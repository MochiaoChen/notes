在[[游戏人生]]中我们介绍了对于多臂老虎机问题的UCB解，在[这篇论文][[多臂老虎机.pdf]中有着详细的解释。本附录做一个初步的推导，只专注于论文中UCB1的方案。

因为笔者没有系统深度学习过进阶的概率论与数理统计，所有的错误均归于AI幻觉和作者的才疏学浅
## 背景介绍

考虑一个基本的多臂老虎机问题(Multi-armed Bandit problem)。假设有 $K$ 台机器，每台机器每次被拉动后会产生一个范围在 0 到 1 之间的随机奖励 。不同机器之间的奖励是互相独立的，并且同一台机器每次的奖励服从一个未知的固定概率分布 。

记第 $i$ 台机器的真实期望收益被定义为 $\mu_i$ 。假设其中期望收益最高的值为 $\mu^{*}$ 。

## 遗憾值

我们衡量一个操作策略性能的指标为遗憾值(Regret)， 其定义如下

假设在总共  $n$  次操作中，策略选择拉动机器  $j$  的总次数记为 $T_j(n)$

若每次都能选到最优机器，在 $n$ 次操作后的总期望收益就是 $\mu^{*} n$ 。

事实上策略会在不同机器上进行选择，设第 $j$ 台机器被拉动了 $T_j(n)$ 次，其期望收益为  $\mu_j E[T_j(n)]$  。 $K$ 台机器的的总期望收益为
$$\sum_{j=1}^K \mu_j E[T_j(n)]$$

用理想收益减去实际收益，即得到了遗憾值的定义式：
$$Regret = \mu^* n - \sum_{j=1}^K \mu_j E[T_j(n)]$$
由于共操作了 $n$ 次，即所有机器被拉动的次数总和必定等于 $n$。则有
$$\sum_{j=1}^K T_j(n) = n$$
两边取期望 , 得:

$$\sum_{j=1}^K E[T_j(n)] = n$$
代入上式:
$$Regret = \mu^* \sum_{j=1}^K E[T_j(n)] - \sum_{j=1}^K \mu_j E[T_j(n)]$$
提取公因式得:
$$Regret = \sum_{j=1}^K (\mu^* - \mu_j) E[T_j(n)]$$
我们定义$\Delta_j = \mu^* - \mu_j$，为机器 $j$ 与最优机器期望收益的差。

注意到当机器 $j$ 本身为最优机器时，$\mu_j = \mu^{*}$， 即$\Delta_j = 0$。我们只需要讨论所有满足$\mu_j < \mu^*$的机器。

即得：
$$Regret = \sum_{j:\mu_j < \mu^*} \Delta_j E[T_j(n)]$$

## Chernoff-Hoeffding 边界

由Markov 不等式：
$$P(Y \ge a) \le \frac{E[Y]}{a}$$
其中$Y$是任意非负的随机变量，$a > 0$。

为了得到更紧的边界，Chernoff 引入了将随机变量 $X$ 放到指数上，并引入一个任意大于零的参数 $\lambda$ 的技巧。

我们要计算样本总和$S_n$ 偏离其期望 $E[S_n]$ 超过 $a$ 的概率。利用指数函数的单调性：
$$P(S_n - E[S_n] \ge a) = P(e^{\lambda(S_n - E[S_n])} \ge e^{\lambda a})$$
对右侧应用Markov 不等式：
$$P \le \frac{E[e^{\lambda(S_n - E[S_n])}]}{e^{\lambda a}}$$
由于这 $n$ 个样本 $X_i$ 是相互独立的，总和的指数期望可以拆分成每个样本指数期望的乘积：
$$P \le e^{-\lambda a} \prod_{i=1}^n E[e^{\lambda(X_i - E[X_i])}]$$
如何处理$E[e^{\lambda(X_i - E[X_i])}]$ ?  Hoeffding 提出了一个引理：假设有一个均值为 0 的随机变量 $Z$，且它的取值范围严格落在 $[c, d]$ 之间。那么它的矩母函数满足：
$$E[e^{\lambda Z}] \le e^{\frac{\lambda^2(d-c)^2}{8}}$$
这里的证明留做习题，请读者自证。参考的证明见文末。

在我们的问题中，单次机器收益 $X_i$ 的取值范围是 $[0, 1]$ 。我们令 $Z_i = X_i - E[X_i]$，显然 $Z_i$ 的均值为 0。因为 $X_i$ 最大是 1，最小是 0，所以 $Z_i$ 的取值区间长度 $(d-c)$ 正好等于 $1$。

代入Hoeffding 引理， 得：
$$E[e^{\lambda Z_i}] \le e^{\frac{\lambda^2}{8}}$$
带入前式，注意这里有$n$ 个独立的样本，即需要乘 $n$ 次，得：

$$P \le e^{-\lambda a} \prod_{i=1}^n e^{\frac{\lambda^2}{8}} = e^{-\lambda a + \frac{n\lambda^2}{8}}$$

该不等式对于任何大于零的 $\lambda$ 都成立。我们需要找到一个$\lambda$，使得等式右边的值尽可能小。

对于$f(\lambda) = -\lambda a + \frac{n\lambda^2}{8}$，求一阶导，并令导数为$0$：

$$-a + \frac{n\lambda}{4} = 0$$
得：$$\lambda = \frac{4a}{n}$$
带入前式，有：
$$-\left(\frac{4a}{n}\right)a + \frac{n}{8}\left(\frac{4a}{n}\right)^2 = -\frac{4a^2}{n} + \frac{2a^2}{n} = -\frac{2a^2}{n}$$
即得：

$$P(S_n - E[S_n] \ge a) \le e^{-\frac{2a^2}{n}}$$
## 确信界
在老虎机问题中，我们将$S_n$转换为经验均值 $\bar{X}_n = \frac{S_n}{n}$，并令偏差比例为 $\epsilon = \frac{a}{n}$。

带入上面的不等式，即得到描述经验均值偏离真实期望的概率形式：
$$P\{\bar{X}_n \ge \mu + \epsilon\} \le e^{-2n\epsilon^2}$$

在算法中，设计者需要设定一个动态变化的置信区间半径 $c_{t,s}$ ，$t$ 代表当前的总操作步数，$s$ 代表某台特定机器当前被拉动的次数 。为了保证最终的总遗憾值能够收敛到一个较小的数值，我们需要把“机器的经验均值偏离真实期望超出 $c_{t,s}$”这个错误事件的发生概率控制在一个极小的量级。按照原作者的思路，我们选择目标衰减概率是 $t^{-4}$ 。选择 4 次方的原因是 $\sum t^{-4}$ 这个无穷级数求和后能够收敛到一个极小的常数。

将参数代入刚才我们转化好的公司中。为确保错误概率的上限刚好等于预设的 $t^{-4}$，我们建立如下等式：$$e^{-2sc_{t,s}^2} = t^{-4}$$
对等式两边同时取自然对数：
$$-2sc_{t,s}^2 = -4 \ln t$$
移项可得：
$$c_{t,s}^2 = \frac{4 \ln t}{2s}$$
则有：
$$c_{t,s} = \sqrt{\frac{2 \ln t}{s}}$$

## 操作策略

策略 UCB1 评估机器的标准是由两部分组成的指数：当前经验平均收益，加上基于霍夫丁边界(Chernoff-Hoeffding bound)计算的置信区间大小 。在总共进行到第 $n$ 步时，算法会选择让以下公式最大化的机器 $j$：

$$\bar{x}_j + \sqrt{\frac{2 \ln n}{n_j}}$$
$\bar{x}_j$ 是机器 $j$ 的历史平均收益，$n_j$ 是机器 $j$ 被拉动的次数 。根据前面总遗憾值的推导，我们需要约束次优机器被选中的次数上限。

假设在第 $t$ 步，算法错误地选择了某台次优机器 $i$。这说明在这一时刻，次优机器 $i$ 的置信上限评估值超过了最优机器(用 $*$ 表示)的置信上限 。具体表现为如下不等式成立：
$$\bar{X}^{*}_{T^{*}(t-1)} + c_{t-1, T^*(t-1)} \le \bar{X}_{i, T_i(t-1)} + c_{t-1, T_i(t-1)}$$
当上述不等式成立时，以下三个事件中必定有至少一个发生 ：
- 事件 1：最优机器的经验均值大幅度低于其真实期望，即 $\bar{X}^*_{s} \le \mu^* - c_{t,s}$ 。
    
- 事件 2：次优机器 $i$ 的经验均值大幅度高于其真实期望，即 $\bar{X}_{i,s_i} \ge \mu_i + c_{t,s_i}$ 。
    
- 事件 3：次优机器 $i$ 的真实期望加上两倍的置信区间余量，依然大于最优机器的真实期望，即 $\mu^* < \mu_i + 2 c_{t,s_i}$ 。

对于事件1与事件2，我们已知：
$$P\{\bar{X}^*_s \le \mu^* - c_{t,s}\} \le e^{-4 \ln t} = t^{-4}$$

$$P\{\bar{X}_{i,s_i} \ge \mu_i + c_{t,s_i}\} \le e^{-4 \ln t} = t^{-4}$$
对于事件3，带入$c_{t,s_i}$ 与期望差的记号$\Delta_i$，可以得到：
$$s_i < \frac{8 \ln t}{\Delta_i^2}$$
由于整体步数最大为 $n$，我们可以设定一个阈值 $l = \lceil \frac{8 \ln n}{\Delta_i^2} \rceil$ 。只要次优机器 $i$ 被拉动的次数达到了这个阈值 $l$，事件 3 的不等式就绝对无法成立，即事件 3 为假 。

我们将次优机器 $i$ 所有的拉动次数加总求期望 。我们将这分为两部分：无条件发生的前 $l$ 次，以及在拉动次数超过 $l$ 后由事件 1 和事件 2 引起的小概率额外拉动次数：
$$E[T_i(n)] \le \lceil \frac{8 \ln n}{\Delta_i^2} \rceil + \sum_{t=1}^{\infty} \sum_{s=1}^{t} \sum_{s_i=1}^{t} ( P\{\bar{X}^*_s \le \mu^* - c_{t,s}\} + P\{\bar{X}_{i,s_i} \ge \mu_i + c_{t,s_i}\} )$$
将之前取的极小概率$t^{-4}$代入后方的无穷级数求和中，利用数学分析中级数求和的技巧，可以将其化简为常数项 $\frac{\pi^2}{3}$ 。

因此，次优机器被拉动次数的期望上限被严格证明为：
$$\frac{8 \ln n}{\Delta_i^2} + 1 + \frac{\pi^2}{3}$$

总结来说，具体操作分为两个固定阶段：

1. **初始化阶段**：在游戏刚开始时，算法会把所有 $K$ 台机器都依次拉动一次 。这保证了每台机器都有一个初始的经验均值数据。

2. **主循环阶段**：在之后的每一次选择中，算法会为每一台机器计算一个评估指数 。这个指数由两部分相加构成：第一部分是这台机器目前的平均收益 $\bar{x}_j$ ；第二部分就是置信半径 $\sqrt{\frac{2 \ln n}{n_j}}$ 。在这个公式中，$n$ 代表当前总共进行的轮数，$n_j$ 代表这台特定机器 $j$ 已经被拉动的次数 。算法每一次都会挑出这个总指数最大的那一台机器去拉动 。

下面是用Python实现的一个模拟代码

```Python
import math

class UCB1:
    def __init__(self, n_arms):
        self.n_arms = n_arms
        self.counts = [0] * n_arms
        self.values = [0.0] * n_arms
        self.total_counts = 0

    def select_arm(self):
        for arm in range(self.n_arms):
            if self.counts[arm] == 0:
                return arm

        ucb_values = [0.0] * self.n_arms
        for arm in range(self.n_arms):
            bonus = math.sqrt((2 * math.log(self.total_counts)) / float(self.counts[arm]))
            ucb_values[arm] = self.values[arm] + bonus

        return ucb_values.index(max(ucb_values))

    def update(self, chosen_arm, reward):
        self.counts[chosen_arm] += 1
        self.total_counts += 1

        n = self.counts[chosen_arm]
        value = self.values[chosen_arm]
        new_value = ((n - 1) / float(n)) * value + (1 / float(n)) * reward
        self.values[chosen_arm] = new_value
```

---
## Hoeffding's Lemma 的证明

由于函数 $f(x) = e^{\lambda x}$ 在实数域上是凸函数，对于区间 $[c, d]$ 中的任何点 $Z$，我们可以利用凸组合将其表示为端点的线性组合。

具体来说，对于 $Z \in [c, d]$，可以写成：

$$Z = \frac{d-Z}{d-c} \cdot c + \frac{Z-c}{d-c} \cdot d$$

利用凸性不等式可得：

$$e^{\lambda Z} \le \frac{d-Z}{d-c} e^{\lambda c} + \frac{Z-c}{d-c} e^{\lambda d}$$
已知随机变量满足 $E[Z] = 0$。对上述不等式两边同时取期望：

$$E[e^{\lambda Z}] \le \frac{d-E[Z]}{d-c} e^{\lambda c} + \frac{E[Z]-c}{d-c} e^{\lambda d}$$

代入 $E[Z] = 0$ 后简化为：

$$E[e^{\lambda Z}] \le \frac{d}{d-c} e^{\lambda c} - \frac{c}{d-c} e^{\lambda d}$$
设 $p = \frac{-c}{d-c}$。由于 $c \le E[Z] \le d$ 且 $E[Z]=0$，必然有 $c \le 0 \le d$，因此 $p$ 处于 $[0, 1]$ 区间内。

此时 $1-p = \frac{d}{d-c}$。令 $u = \lambda(d-c)$，不等式右侧可以写成：

$$\phi(u) = (1-p)e^{-pu} + pe^{(1-p)u} = e^{-pu}(1-p+pe^u)$$

我们现在的目标是证明
$$\ln \phi(u) \le \frac{u^2}{8}$$
定义函数 $g(u) = \ln \phi(u) = -pu + \ln(1-p+pe^u)$。计算其前二阶导数：
$g(0) = \ln(1) = 0$

 $$g'(u) = -p + \frac{pe^u}{1-p+pe^u}$$代入得 $g'(0) = 0$
 
$$g''(u) = \frac{p(1-p)e^u}{(1-p+pe^u)^2}$$
令 $A = 1-p$，$B = pe^u$，则
$$g''(u) = \frac{AB}{(A+B)^2}$$
由AM-GM不等式:
$$g''(u) \le \frac{1}{4}$$
由Taylor公式,在0 附近存在某个 $\xi$ 满足：
$$g(u) = g(0) + g'(0)u + \frac{1}{2}g''(\xi)u^2 \le 0 + 0 + \frac{1}{8}u^2$$
带入$u = \lambda(d-c)$, 得:
$$\ln E[e^{\lambda Z}] \le g(u) \le \frac{\lambda^2(d-c)^2}{8}$$
两边取指数函数,即得原命题:

$$E[e^{\lambda Z}] \le e^{\frac{\lambda^2(d-c)^2}{8}}$$
